{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noções básicas de engenharia de *prompt*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quando se está construindo soluções baseadas em LLMs (*Large Language Models*), uma das principais etapas é a engenharia de *prompt* (*prompt engineering*), que consiste na criação cuidadosa de instruções ou de perguntas (*prompts*) para os grandes modelos de linguagem, como GPT (*Generative Pretrained Transformer*) da OpenAI ou Gemini, do Google. O principal objetivo da engenharia de *prompt* é maximizar a eficácia e a precisão dos modelos ao executar suas tarefas, seja responder perguntas ou gerar conteúdo, por exemplo. Esta é uma das formas mais eficazes de melhorar a qualidade dos resultados dos LLMs e é particularmente relevante em modelos que dependem de interações baseadas em texto para entender e executar solicitações dos usuários.\n",
    "\n",
    "Mas, a engenharia de *prompt* não se resume a busca por melhorar a qualidade das respostas dos modelos. Ela pode ser usada para controlar adequadamente suas saídas, seja quanto ao seu formato/padrão, ou com relação às políticas e regras da organização, reduzir o custo/latência das respostas, etc.\n",
    "\n",
    "Abaixo está uma lista de melhores práticas para se realizar a tarefa de engenharia de *prompt* de maneira eficaz e desenvolver bons *prompts* para seus modelos: \n",
    "\n",
    "1. **Especificidade e clareza:** Evite instruções ambíguas. Seja o mais explícito possível em suas solicitações para garantir resultados precisos e relevantes.\n",
    "2. **Use delimitadores:** Os delimitadores ajudam a estruturar e esclarecer os *prompts*, especialmente ao lidar com múltiplas seções de informações.\n",
    "3. **Especifique o formato de saída:** Indique explicitamente o formato desejado da saída, seja texto livre, markdown, HTML ou um formato estruturado, como JSON.\n",
    "4. **Pense passo a passo:** Instruir o modelo a raciocinar passo a passo sobre um problema pode melhorar o desempenho, especialmente em tarefas que envolvem raciocínio complexo.\n",
    "5. **Interpretação de papéis (*Role-Playing*):** Implementar *role-playing* em *prompts*, onde o modelo assume um papel específico, como um assistente de pesquisa, pode melhorar a qualidade e a relevância das respostas.\n",
    "\n",
    "Além disso, diversas técnicas avançadas de engenharia de prompts foram desenvolvidas para aprimorar ainda mais este processo, incluindo:\n",
    "\n",
    "- **Encadeamento de *prompts*:** Em tarefas complexas o LLM pode ter dificuldade em resolver se for solicitado com um único prompt muito detalhado. Uma alternativa que pode melhorar significativamente o seu desempenho é dividir a tarefa em várias subtarefas mais simples, onde o LLM recebe uma subtarefa e sua resposta é usada como entrada para outro prompt, criando uma cadeia de operações de prompts.\n",
    "- **Aprendizado de poucos exemplos em contexto (*Few-shot in-context learning*):** Esta técnica envolve fornecer ao LLM alguns exemplos do tipo de resposta desejada dentro do *prompt*, ajudando o modelo a entender melhor o contexto e a produzir resultados semelhantes.\n",
    "- **Cadeia de Pensamento (*Chain-of-Thought*):** Com essa abordagem, os *prompts* são elaborados para encorajar o LLM a explicar o raciocínio por trás de suas respostas, passo a passo, como se estivesse \"pensando alto\".\n",
    "- ***ReAct*:** Uma técnica que foca na reação do LLM a diferentes estímulos ou instruções dentro do *prompt*, ajustando a abordagem com base na resposta obtida.\n",
    "\n",
    "Abaixo são apresentados exemplos para alguns dos pontos listados, usando o modelo gpt-3.5-turbo da OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import IPython\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para gerar a resposta do modelo\n",
    "def get_completion(messages, model=\"gpt-3.5-turbo\", temperature=0, max_tokens=300):\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens,\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 - Seja específico e claro\n",
    "Escreva instruções tão claras e específicas quanto possível para obter os comportamentos desejados do LLM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recomendo experimentar a \"Paella Valenciana\". É um prato espanhol delicioso, feito com arroz, frutos do mar, frango, legumes e temperos especiais. Tenho certeza de que você vai adorar!\n"
     ]
    }
   ],
   "source": [
    "menu = [\n",
    "    \"Feijoada Completa\",\n",
    "    \"Ratatouille\",\n",
    "    \"Paella Valenciana\",\n",
    "    \"Sushi\",\n",
    "    \"Pasta Carbonara\",\n",
    "    \"Tom Yum Goong\",\n",
    "    \"Churrasco\",\n",
    "    \"Moussaka\",\n",
    "    \"Biryani\",\n",
    "    \"Tacos\"\n",
    "]\n",
    "\n",
    "system_message = \"\"\"\n",
    "Sua tarefa é recomendar um prato para um cliente.\n",
    "\n",
    "Você é responsável por recomendar um prato do nosso menu {menu}.\n",
    "\n",
    "Você deve evitar pedir preferências ao usuário e não solicitar informações pessoais.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "user_request = \"\"\"\n",
    "Por favor, me recomende um prato do seu menu.\n",
    "\"\"\"\n",
    "\n",
    "message = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": system_message.format(menu=menu)\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": user_request\n",
    "    }\n",
    "]\n",
    "\n",
    "response = get_completion(message)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quanto mais específico for o comportamento desejado do modelo, mais específicas deverão ser as instruções e a lógica. Abaixo está um exemplo em que o cliente fornece informações sobre seus gostos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Com base em seus gostos por comida italiana, eu recomendaria o prato Pasta Carbonara do nosso menu. É um prato clássico e delicioso que combina massa, ovos, queijo parmesão, pancetta e pimenta preta. Tenho certeza de que você vai adorar!\n"
     ]
    }
   ],
   "source": [
    "menu = [\n",
    "    \"Feijoada Completa\",\n",
    "    \"Ratatouille\",\n",
    "    \"Paella Valenciana\",\n",
    "    \"Sushi\",\n",
    "    \"Pasta Carbonara\",\n",
    "    \"Tom Yum Goong\",\n",
    "    \"Churrasco\",\n",
    "    \"Moussaka\",\n",
    "    \"Biryani\",\n",
    "    \"Tacos\"\n",
    "]\n",
    "\n",
    "\n",
    "system_message = \"\"\"\n",
    "Sua tarefa é recomendar um prato para um cliente.\n",
    "\n",
    "Você é responsável por recomendar um prato do nosso menu {menu}.\n",
    "\n",
    "Você deve evitar pedir preferências ao usuário e não solicitar informações pessoais.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "user_request = \"\"\"\n",
    "Eu adoro comida italiana. Por favor, me recomende um prato do seu menu.\n",
    "\"\"\"\n",
    "\n",
    "message = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": system_message.format(menu=menu)\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": user_request\n",
    "    }\n",
    "]\n",
    "\n",
    "response = get_completion(message)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 - Adicionar delimitadores\n",
    "Adicionar delimitadores ajuda a estruturar melhor as instruções e os componentes gerais do *prompt*. Isso é benéfico para obter respostas mais confiáveis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```pythonstrings2.append(\"um\")\n",
       "strings2.append(\"DOIS\")\n",
       "strings2.append(\"7\")\n",
       "strings2.append(\"4\")\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Transforme o bloco de código apresentado na seção #### <code> #### para a linguagem Python:\n",
    "\n",
    "####\n",
    "strings2.push(\"um\")\n",
    "strings2.push(\"DOIS\")\n",
    "strings2.push(\"7\")\n",
    "strings2.push(\"4\")\n",
    "####\n",
    "\"\"\"\n",
    "\n",
    "message = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompt\n",
    "    }\n",
    "]\n",
    "\n",
    "IPython.display.Markdown(\"```python\" + get_completion(message) + \"\\n```\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 - Especifique o formato de saída\n",
    "Se o formato das respostas for importante, isso deverá ser explicitamente declarado no *prompt* para obter os resultados desejados. No exemplo a seguir, gostaríamos de exportar os resultados como um objeto JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"product_name\": \"Nike Air Max 270 React\",\n",
      "    \"product_brand\": \"Nike\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Sua tarefa é: dada a descrição do produto, retornar as informações solicitadas na seção delimitada por ### ###.\n",
    "Formate a saída como um objeto JSON.\n",
    "\n",
    "Descrição do produto: {description}\n",
    "\n",
    "###\n",
    "product_name: o nome do produto\n",
    "product_brand: o nome da marca (se houver)\n",
    "###\n",
    "\"\"\"\n",
    "\n",
    "description = \"\"\"\n",
    "Apresentando o Nike Air Max 270 React: um tênis confortável e estiloso que combina duas das melhores tecnologias\n",
    "da Nike. Com um design preto elegante e uma sola em bolha exclusiva, esses sapatos são perfeitos\n",
    "para o uso diário.\n",
    "\"\"\"\n",
    "\n",
    "message = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompt.format(description=description)\n",
    "    }\n",
    "]\n",
    "\n",
    "print(get_completion(message))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 – Pense passo a passo\n",
    "Para obter raciocínio em LLMs, você pode fazer com que o modelo pense passo a passo. Solicitar ao modelo dessa forma permite que ele forneça as etapas detalhadas antes de fornecer uma resposta final que resolva o problema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Identificar os números ímpares no grupo: 15, 32, 5, 13, 82, 7, 1\n",
      "   Números ímpares: 15, 5, 13, 7, 1\n",
      "\n",
      "2. Somar os números ímpares: 15 + 5 + 13 + 7 + 1 = 41\n",
      "\n",
      "3. Verificar se a soma dos números ímpares é par ou ímpar:\n",
      "   41 é um número ímpar\n",
      "\n",
      "Portanto, a soma dos números ímpares neste grupo é um número ímpar.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Os números ímpares neste grupo, 15, 32, 5, 13, 82, 7, 1, somam um número par?\n",
    "\n",
    "Resolva dividindo o problema em etapas. Identifique os números ímpares, depois some-os e, por fim, indique se\n",
    "o resultado é ímpar ou par.\n",
    "\"\"\"\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": prompt\n",
    "    }\n",
    "]\n",
    "\n",
    "response= get_completion(messages)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 - Interpretação de papéis\n",
    "O exemplo abaixo mostra como aplicar a interpretação de papéis. Nesse caso, o modelo foi induzido a assumir o papel de um assistente de pesquisa, com o objetivo de responder às questões científicas dos usuários. Você pode também combinar diferentes mensagens para imitar ou iniciar o comportamento que deseja ou espera do modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Criação de Buracos Negros\n",
      "\n",
      "Os buracos negros são formados a partir do colapso gravitacional de uma estrela massiva no final de sua vida. Quando uma estrela esgota seu combustível nuclear, a pressão gerada pela fusão nuclear não consegue mais contrabalancear a força da gravidade, levando a um colapso gravitacional.\n",
      "\n",
      "Durante o colapso, a estrela massiva pode se tornar uma supernova, liberando uma quantidade imensa de energia. Se a massa remanescente após a explosão for grande o suficiente, a matéria restante pode colapsar ainda mais, formando um buraco negro.\n",
      "\n",
      "O buraco negro é caracterizado por uma região de espaço-tempo onde a gravidade é tão intensa que nem mesmo a luz consegue escapar, formando assim o horizonte de eventos. Toda a massa da estrela colapsada fica concentrada em um ponto de densidade infinita, chamado de singularidade.\n",
      "\n",
      "Os buracos negros são objetos extremamente fascinantes e desempenham um papel fundamental na astrofísica e na compreensão do universo. Suas propriedades únicas desafiam nossa compreensão da física e da natureza do espaço-tempo.\n"
     ]
    }
   ],
   "source": [
    "system_message = \"\"\"\n",
    "Você é um assistente de pesquisa experiente. Suas respostas dever ter sempre um tom técnico e científico.\n",
    "Estruture suas respostas em formato markdown. \n",
    "\"\"\"\n",
    "\n",
    "prompt = \"\"\"\n",
    "Você pode me contar sobre a criação de buracos negros?\n",
    "\"\"\"\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": system_message\n",
    "    },\n",
    "\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompt\n",
    "    }\n",
    "]\n",
    "\n",
    "response = get_completion(messages)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlops",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
